# -*- coding: utf-8 -*-
"""TechDome_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sk0C3WsHf5-FxgF9QVQ8SquFPmpSw8A5
"""

!pip install sumy

!pip install pyate

!pip install rake-nltk
from rake_nltk import Rake

import spacy
from pyate import combo_basic
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from gensim import corpora
from gensim.models import LdaModel
from collections import defaultdict
import pandas as pd
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from collections import defaultdict
from rake_nltk import Rake
from sklearn.metrics import precision_score, recall_score, f1_score

# Download NLTK data files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
# Load spacy model
nlp = spacy.load("en_core_web_sm")

df = pd.read_excel("Assignment.xlsx")
df

def clean_article(article):
    # Tokenize the article
    words = word_tokenize(article)

    # Remove punctuation and convert to lowercase
    words = [word.lower() for word in words if word.isalpha()]

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    cleaned_article = ' '.join(words)
    return cleaned_article

# Clean articles and add a new column 'clean_text'
df['clean_text'] = df['Article'].apply(clean_article)

# Display the preprocessed DataFrame
df

sid = SentimentIntensityAnalyzer()

# Function to get the overall sentiment of an article
def get_article_sentiment(article_text):
    scores = sid.polarity_scores(article_text)

    # Determine the overall sentiment based on the compound score
    if scores['compound'] >= 0.05:
        return 'positive'
    elif scores['compound'] <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# Apply the sentiment analysis to each article in the DataFrame
df['mood'] = df['clean_text'].apply(get_article_sentiment)
df

def generate_summary(article_text, num_sentences=1):
    # Initialize parser and tokenizer
    parser = PlaintextParser.from_string(article_text, Tokenizer("english"))

    # Initialize LSA Summarizer
    summarizer = LsaSummarizer()

    # Generate summary
    summary = summarizer(parser.document, num_sentences)
    summary_text = ""
    for sentence in summary:
        summary_text += str(sentence) + " "

    return summary_text

# Apply the function to create a summary for each article in the DataFrame
df['summary'] = df['Article'].apply(generate_summary)

df

def find_connections(articles, num_topics=5, num_words=5):
    # Preprocess the articles
    processed_articles = [clean_article(article).split() for article in articles]

    # Create a dictionary and corpus
    dictionary = corpora.Dictionary(processed_articles)
    corpus = [dictionary.doc2bow(text) for text in processed_articles]

    # Train the LDA model
    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)

    # Print the topics
    topics = lda_model.print_topics(num_words=num_words)
    for idx, topic in topics:
        print(f"Topic {idx}: {topic}")

find_connections(df['clean_text'].tolist())

def aspect_analysis(article, max_aspects=1):
    # Initialize RAKE and VADER Sentiment Analyzer
    r = Rake()
    analyzer = SentimentIntensityAnalyzer()

    # Extract aspects using RAKE
    r.extract_keywords_from_text(article)
    aspects = r.get_ranked_phrases()

    # Initialize variables to track the highest sentiment score and corresponding aspect
    max_sentiment_score = -1  # Initialize to a low value
    top_aspect = None

    # Tokenize sentences
    sentences = nltk.sent_tokenize(article)

    # Process each sentence and aspect
    for sentence in sentences:
        for aspect in aspects:
            if aspect in sentence:
                sentiment = analyzer.polarity_scores(sentence)['compound']
                if sentiment > max_sentiment_score:
                    max_sentiment_score = sentiment
                    top_aspect = aspect

    # Return the top aspect (if found)
    return {top_aspect: round(max_sentiment_score, 2)} if top_aspect else {}

# Iterate over each row in the DataFrame and apply aspect analysis function
for index, row in df.iterrows():
    article_text = row['Article']
    aspect_summary = aspect_analysis(article_text, max_aspects=1)
    aspect_string = str(aspect_summary)
    df.at[index, 'aspect'] = aspect_string

# Print the DataFrame with the extracted aspects
df

# Save the updated DataFrame to a new Excel file
df.to_excel('new_updated_assignment.xlsx', index=False)

